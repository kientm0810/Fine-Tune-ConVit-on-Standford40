{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12013061,"sourceType":"datasetVersion","datasetId":7557582}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T04:14:05.908376Z","iopub.execute_input":"2025-06-02T04:14:05.908723Z","iopub.status.idle":"2025-06-02T04:14:05.913478Z","shell.execute_reply.started":"2025-06-02T04:14:05.908699Z","shell.execute_reply":"2025-06-02T04:14:05.912756Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ======================= 1. Install & Import ======================\n# import torch\n# print(\"Torch version:\", torch.__version__)\n# print(\"CUDA available:\", torch.cuda.is_available())\n# print(\"CUDA device count:\", torch.cuda.device_count())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T04:14:05.914771Z","iopub.execute_input":"2025-06-02T04:14:05.914978Z","iopub.status.idle":"2025-06-02T04:14:05.928387Z","shell.execute_reply.started":"2025-06-02T04:14:05.914963Z","shell.execute_reply":"2025-06-02T04:14:05.927612Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport timm\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\n# from torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torch.optim import SGD\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom tqdm import tqdm\nimport torch.multiprocessing as mp\n# from dataset import Stanford40Dataset\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n# Class\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\n# for report\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T04:14:05.929035Z","iopub.execute_input":"2025-06-02T04:14:05.929235Z","iopub.status.idle":"2025-06-02T04:14:17.528686Z","shell.execute_reply.started":"2025-06-02T04:14:05.929221Z","shell.execute_reply":"2025-06-02T04:14:17.528089Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ======================= 2. Custom Dataset ========================\nclass Stanford40Dataset(Dataset):\n    def __init__(self, img_dir, split_file, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        with open(split_file) as f:\n            # danh sách tên ảnh (không .jpg)\n            self.names = [l.strip().split(\".\")[0] for l in f if l.strip().split(\".\")[0]]\n        # build list nhãn đầy đủ\n        labels = sorted({self._label_from_name(n) for n in self.names})\n        self.cls2idx = {c:i for i,c in enumerate(labels)}\n\n    def _label_from_name(self, name):\n        parts = name.split(\"_\")\n        return \"_\".join(parts[:-1])  # hoặc \" \".join(parts[:-1])\n\n    def __len__(self):\n        return len(self.names)\n\n    def __getitem__(self, idx):\n        name = self.names[idx]\n        img_path = os.path.join(self.img_dir, name + \".jpg\")\n        img = Image.open(img_path).convert(\"RGB\")\n        label_str = self._label_from_name(name)\n        label = self.cls2idx[label_str]\n        if self.transform:\n            img = self.transform(img)\n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T04:14:17.530212Z","iopub.execute_input":"2025-06-02T04:14:17.530585Z","iopub.status.idle":"2025-06-02T04:14:17.537555Z","shell.execute_reply.started":"2025-06-02T04:14:17.530567Z","shell.execute_reply":"2025-06-02T04:14:17.536796Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ======================= 3. Hyperparams & Transforms ==============\nNUM_CLASSES = 40\nBATCH_SIZE = 16\nLR = 1e-4 # Learning Rate\nWD = 2e-2 # Weigth decay\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ColorJitter(\n        brightness=0.4, \n        contrast=0.4, \n        saturation=0.4, \n        hue=0.1\n    ),\n    transforms.RandomRotation(degrees=10),\n    transforms.RandAugment(num_ops=2, magnitude=9),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=(0.485, 0.456, 0.406), \n        std=(0.229, 0.224, 0.225)\n    ),\n    transforms.RandomErasing(\n        p=0.5, \n        scale=(0.02, 0.33), \n        ratio=(0.3, 3.3), \n        value='random'\n    ),\n])\nval_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225)),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T04:14:17.538275Z","iopub.execute_input":"2025-06-02T04:14:17.538514Z","iopub.status.idle":"2025-06-02T04:14:17.618127Z","shell.execute_reply.started":"2025-06-02T04:14:17.538498Z","shell.execute_reply":"2025-06-02T04:14:17.617457Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ======================= 4. DataLoader ============================\nDATA_ROOT = \"/kaggle/input/stanford40\"\ntrain_ds = Stanford40Dataset(\n    img_dir=os.path.join(DATA_ROOT, \"JPEGImages\"),\n    split_file=os.path.join(DATA_ROOT, \"ImageSplits\", \"train.txt\"),\n    transform=train_transform)\nval_ds = Stanford40Dataset(\n    img_dir=os.path.join(DATA_ROOT, \"JPEGImages\"),\n    split_file=os.path.join(DATA_ROOT, \"ImageSplits\", \"test.txt\"),\n    transform=val_transform)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\nprint(train_loader.__len__)\nprint(val_loader.__len__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T04:14:17.619198Z","iopub.execute_input":"2025-06-02T04:14:17.619515Z","iopub.status.idle":"2025-06-02T04:14:17.669525Z","shell.execute_reply.started":"2025-06-02T04:14:17.619490Z","shell.execute_reply":"2025-06-02T04:14:17.668882Z"}},"outputs":[{"name":"stdout","text":"<bound method DataLoader.__len__ of <torch.utils.data.dataloader.DataLoader object at 0x783f38a44790>>\n<bound method DataLoader.__len__ of <torch.utils.data.dataloader.DataLoader object at 0x783f38a6fa50>>\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# =============== 5. Model ===============\n# model = timm.create_model(\"convit_base\", pretrained=True)\n\n# # Freeze toàn bộ parameters trước\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# # Thay classifier head\n# in_features = model.head.in_features\n# model.head = nn.Linear(in_features, NUM_CLASSES)\n\n# # Head mới có requires_grad=True theo mặc định\n# model = model.to(DEVICE)\nmodel = timm.create_model(\"convit_base\", pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze 2 block Transformer (10, 11)\nfor idx in [10, 11]:\n    for param in model.blocks[idx].parameters():\n        param.requires_grad = True\n\nin_features = model.head.in_features\nmodel.head = nn.Linear(in_features, NUM_CLASSES)\nmodel = model.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T04:14:17.670217Z","iopub.execute_input":"2025-06-02T04:14:17.670468Z","iopub.status.idle":"2025-06-02T04:14:20.639020Z","shell.execute_reply.started":"2025-06-02T04:14:17.670448Z","shell.execute_reply":"2025-06-02T04:14:20.638406Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b4bdc8422e4069bac3f78c61d53ec2"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# ======================= 6. Optimizer, Scheduler, Loss ===========\noptimizer = torch.optim.SGD(model.parameters(), momentum = 0.9,\n                               lr=LR, weight_decay= WD)\n# cosine + 5-epoch warm-up\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n              optimizer, T_max=30,\n              eta_min=1e-6)\n\ncriterion = nn.CrossEntropyLoss()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T04:14:20.639719Z","iopub.execute_input":"2025-06-02T04:14:20.639926Z","iopub.status.idle":"2025-06-02T04:14:20.645385Z","shell.execute_reply.started":"2025-06-02T04:14:20.639909Z","shell.execute_reply":"2025-06-02T04:14:20.644657Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ======================= 7. Training Loop ========================\ntrain_losses = []\nval_losses   = []\nval_accs     = []\n\nNUM_EPOCHS = 30 # try 30 epoch when freeze head\n\nbest_acc = 0.0\nfor epoch in range(1, NUM_EPOCHS + 1):\n    # ======== Training ========\n    model.train()\n    running_train_loss = 0.0\n    num_train_samples = 0\n\n    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [Train]\"):\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        batch_size = imgs.size(0)\n        running_train_loss += loss.item() * batch_size\n        num_train_samples += batch_size\n\n    avg_train_loss = running_train_loss / num_train_samples\n    train_losses.append(avg_train_loss)\n    scheduler.step()\n\n    # ======== Validation ========\n    model.eval()\n    running_val_loss = 0.0\n    num_val_samples = 0\n    correct = 0\n\n    with torch.no_grad():\n        for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [ Val ]\"):\n            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n\n            batch_size = imgs.size(0)\n            running_val_loss += loss.item() * batch_size\n            num_val_samples += batch_size\n\n            preds = outputs.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n\n    avg_val_loss = running_val_loss / num_val_samples\n    val_losses.append(avg_val_loss)\n\n    acc = correct / num_val_samples\n    val_accs.append(acc)\n\n    print(\n        f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n        f\"Train Loss: {avg_train_loss:.4f} | \"\n        f\"Val Loss: {avg_val_loss:.4f} | \"\n        f\"Val Acc: {acc*100:.2f}%\"\n    )\n\n    # ======== Save best ========\n    if acc > best_acc:\n        best_acc = acc\n        torch.save(model.state_dict(), \"best_convit_stanford40.pth\")\n\nprint(f\"Best Val Acc only head: {best_acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T04:14:20.646015Z","iopub.execute_input":"2025-06-02T04:14:20.646191Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/30 [Train]: 100%|██████████| 250/250 [00:41<00:00,  5.97it/s]\nEpoch 1/30 [ Val ]: 100%|██████████| 346/346 [00:45<00:00,  7.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 | Train Loss: 3.5783 | Val Loss: 3.3367 | Val Acc: 19.58%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30 [Train]: 100%|██████████| 250/250 [00:41<00:00,  6.09it/s]\nEpoch 2/30 [ Val ]: 100%|██████████| 346/346 [00:44<00:00,  7.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/30 | Train Loss: 3.1588 | Val Loss: 2.8760 | Val Acc: 45.66%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30 [Train]: 100%|██████████| 250/250 [00:40<00:00,  6.10it/s]\nEpoch 3/30 [ Val ]: 100%|██████████| 346/346 [00:44<00:00,  7.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/30 | Train Loss: 2.7372 | Val Loss: 2.4317 | Val Acc: 59.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30 [Train]: 100%|██████████| 250/250 [00:41<00:00,  6.09it/s]\nEpoch 4/30 [ Val ]: 100%|██████████| 346/346 [00:44<00:00,  7.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/30 | Train Loss: 2.3454 | Val Loss: 2.0444 | Val Acc: 67.48%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30 [Train]: 100%|██████████| 250/250 [00:41<00:00,  6.09it/s]\nEpoch 5/30 [ Val ]: 100%|██████████| 346/346 [00:44<00:00,  7.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/30 | Train Loss: 2.0323 | Val Loss: 1.7359 | Val Acc: 71.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30 [Train]: 100%|██████████| 250/250 [00:41<00:00,  6.10it/s]\nEpoch 6/30 [ Val ]: 100%|██████████| 346/346 [00:44<00:00,  7.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/30 | Train Loss: 1.7826 | Val Loss: 1.5077 | Val Acc: 74.28%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30 [Train]: 100%|██████████| 250/250 [00:41<00:00,  6.10it/s]\nEpoch 7/30 [ Val ]: 100%|██████████| 346/346 [00:44<00:00,  7.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/30 | Train Loss: 1.6007 | Val Loss: 1.3383 | Val Acc: 76.19%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30 [Train]: 100%|██████████| 250/250 [00:40<00:00,  6.10it/s]\nEpoch 8/30 [ Val ]: 100%|██████████| 346/346 [00:44<00:00,  7.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/30 | Train Loss: 1.4471 | Val Loss: 1.2123 | Val Acc: 77.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30 [Train]: 100%|██████████| 250/250 [00:40<00:00,  6.10it/s]\nEpoch 9/30 [ Val ]: 100%|██████████| 346/346 [00:44<00:00,  7.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/30 | Train Loss: 1.3321 | Val Loss: 1.1137 | Val Acc: 78.56%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/30 [Train]: 100%|██████████| 250/250 [00:41<00:00,  6.09it/s]\nEpoch 10/30 [ Val ]:   2%|▏         | 6/346 [00:00<00:46,  7.26it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ======================= 8. Loss and Accuracy ========================\nepochs = np.arange(1, NUM_EPOCHS + 1)\n\nplt.figure(figsize=(12,5))\n\n# Loss\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_losses, label=\"Train Loss\")\nplt.plot(epochs, val_losses,   label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss over Epochs\")\nplt.legend()\n\n# Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(epochs, np.array(val_accs)*100, label=\"Val Acc\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Validation Accuracy over Epochs\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======== 9. Confusion Matrix and Classification report ========\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for imgs, labels in tqdm(val_loader, desc=\"Final Eval [Val]\"):\n        imgs = imgs.to(DEVICE)\n        outputs = model(imgs)\n        preds = outputs.argmax(dim=1).cpu().numpy()\n        all_preds.append(preds)\n        all_labels.append(labels.numpy())\n\nall_preds = np.concatenate(all_preds)\nall_labels = np.concatenate(all_labels)\n\n# ======================= 6. Confusion Matrix and heatmap ========================\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for imgs, labels in tqdm(val_loader, desc=\"Final Eval [Val]\"):\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        outputs = model(imgs)\n        preds = outputs.argmax(dim=1).cpu().numpy()\n        all_preds.append(preds)\n        all_labels.append(labels.cpu().numpy())\n\nall_preds = np.concatenate(all_preds)\nall_labels = np.concatenate(all_labels)\n\ncm = confusion_matrix(all_labels, all_preds)\n\n# Draw heatmap confusion matrix with matplotlib\nplt.figure(figsize=(10, 8))\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix (Stage 2)\")\nplt.colorbar()\ntick_marks = np.arange(NUM_CLASSES)\n\nplt.xticks(tick_marks, classes, rotation=90, fontsize=7)\nplt.yticks(tick_marks, classes, fontsize=7)\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.tight_layout()\nplt.show()\n\n# 2) Classification report\n# Cần list tên các class (40 labels) theo đúng thứ tự cls2idx\nclasses = [c for c, idx in sorted(train_ds.cls2idx.items(), key=lambda x: x[1])]\nreport = classification_report(all_labels, all_preds, target_names=classes, digits=4)\nprint(\"=== Classification Report ===\")\nprint(report)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. Quan sát chung**\n**Accuracy chung:** 84.83 % trên toàn bộ 5 532 ảnh validation.\n\n* **Macro F1-score:** khoảng 0.8385, nghĩa là nếu lấy trung bình F1 của 40 lớp (không tính trọng số), mô hình đạt ~83.85 %.\n\n* **Weighted F1-score:** ~0.8495, có phần cao hơn macro, vì các lớp nhiều support (ví dụ “riding_a_bike” 193 ảnh, “walking_the_dog” 193 ảnh,…) có F1 tốt hơn, kéo điểm trung bình lên.\n\nNhìn tổng thể, mô hình hoạt động khá tốt với nhiều lớp F1 ≥ 0.90, nhưng vẫn còn một số lớp F1 rất thấp (dưới 0.60), cần xem xét để cải thiện.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support\n\n# === 9.1. TOP-5 maximum value in confusion matrix ===\n\n# cm : numpy array (40, 40)\n# create list tuple (count, true_idx, pred_idx)\nflat_cm = [\n    (int(cm[i, j]), i, j)\n    for i in range(cm.shape[0])\n    for j in range(cm.shape[1]) if (i != j)\n]\n\n# sort decs\nflat_cm.sort(key=lambda x: x[0], reverse=True)\n\nprint(\"=== Top 20 values in Confusion Matrix ===\")\nfor count, i, j in flat_cm[:20]:\n    print(f\"Count={count:4d} | True = '{classes[i]:<20}' | Pred = '{classes[j]:<20}'\")\n# Eg: Count= 123 | True = 'walking            ' | Pred = 'walking            '","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Phân tích Top-20 giá trị lớn nhất trong Confusion Matrix**\nNhững cặp (true→pred) có count cao cho thấy mô hình thường nhầm lẫn giữa hai lớp đó:\n\n* **(applauding → waving_hands) = 25 lần**\n\n    - “applauding” (vỗ tay) và “waving_hands” (vẫy tay) đều liên quan đến hai bàn tay chuyển động, rất dễ bị nhầm.\n\n    - Khi nhìn một người co bàn tay lên, mô hình khó phân biệt là họ đang vỗ tay hay đang vẫy chào.\n\n* **(phoning → smoking) = 24 lần**\n\n    - “phoning” (đang cầm điện thoại) và “smoking” (đang cầm điếu thuốc) đều có động tác đưa bàn tay lên miệng/gần miệng, bối cảnh tương tự (tay cầm vật nhỏ, ngước mắt nhìn).\n\n    - Nếu background không rõ (ví dụ tay che mặt, màu da, ánh sáng…), model dễ nhầm.\n\n* **(reading → writing_on_a_book) = 23 lần**\n\n    - “reading” (đang đọc sách) và “writing_on_a_book” (đang viết vào sách) đôi khi rất giống: cả hai đều đưa đầu về phía sách, tay bám vào bìa/cuốn sách.\n\n    - Khi ảnh không cho thấy rõ cây bút, model chỉ nhìn thấy “người gập đầu vào sách” nên không phân biệt được.\n\n* **(cooking → washing_dishes) = 19 lần, (cooking → cutting_vegetables) = 16 lần**\n\n    - “cooking” (nấu ăn) thường diễn ra trong bếp, xung quanh có dao thớt, nồi, cả bồn rửa bát…\n\n    - “washing_dishes” (rửa bát) và “cutting_vegetables” (thái rau) đều là một phần của quy trình nấu ăn. Khi ảnh chỉ thấy người đứng cạnh bồn rửa, model dễ gán nhầm thành “washing_dishes” hoặc “cutting_vegetables” thay vì “cooking”.\n\n* **(waving_hands → applauding) = 16 lần**\n\n    - Vỗ tay và vẫy tay thường khó phân biệt chỉ qua một khung tĩnh.\n\n* **Một số cặp nhầm lẫn khác đáng chú ý:**\n\n    - (drinking → pouring_liquid) = 11\n\n    - (reading → texting_message) = 11\n\n    - (smoking → phoning) = 11, (phoning → taking_photos) = 10\n\n    - (running → throwing_frisby) = 10: hai hoạt động này đều có cử động tay chân tương tự, đặc biệt là khi ảnh chụp lúc chạy kèm theo động tác “ném” (tay vung ra).\n\n    - (jumping → running) = 9: đôi khi khó phân biệt tư thế nhảy với tư thế đang chạy chỉ qua một ảnh tĩnh (chân, dáng người).\n\n* **Những cặp trên phản ánh hai vấn đề chính:**\n\n    - Các hành động quá “lý thuyết giống nhau” về tư thế (pose) và bối cảnh (background).\n\n    - Một số lớp thực sự nằm trong cùng tổng thể ngữ cảnh (vd “cooking” bao gồm cắt, rửa, xào, v.v.) dẫn đến nhầm lẫn.\n\n","metadata":{}},{"cell_type":"code","source":"# === 9.2. F1-score for all class and print top10 highest / lowest ===\n\n# precision, recall, f1_scores, support\nprecision, recall, f1_scores, support = precision_recall_fscore_support(\n    all_labels, \n    all_preds,\n    labels=range(len(classes)),\n    zero_division=0  \n)\n\n# list tuple (f1, class_idx)\nf1_with_idx = [(f1_scores[i], i) for i in range(len(classes))]\n\n# Top-10 F1 highest\nf1_with_idx.sort(key=lambda x: x[0], reverse=True)\nprint(\"\\n=== Top 10 highest F1-scores ===\")\nfor f1, idx in f1_with_idx[:10]:\n    print(f\"Class = '{classes[idx]:<20}' | F1 = {f1:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Top-10 F1 lowest\nf1_with_idx.sort(key=lambda x: x[0]) \nprint(\"\\n=== Top 10 lowest F1-scores ===\")\nfor f1, idx in f1_with_idx[:10]:\n    print(f\"Class = '{classes[idx]:<20}' | F1 = {f1:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_with_idx = [(precision[i], recall[i], f1_scores[i], support[i], i) for i in range(len(classes))]\nall_with_idx.sort(key=lambda x: x[2]) \nfor pre, rec, f1, sup, idx in all_with_idx[:10]:\n    print(f\"Class = '{classes[idx]:<20}' | precision = {pre:.4f} | recall = {rec:.4f} | F1 = {f1:.4f} | support = {sup}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. Phân tích Top-10 F1-score thấp nhất**\nNhận xét:\n**waving_hands (F1=0.5645)**\n\n        * Precision chỉ 50.7 %: tức hơn 49 % ảnh mà model gán nhãn “waving_hands” hóa ra là các lớp khác (FP nhiều).\n\n        * Recall 63.6 %: model cũng bỏ sót ~36 % ảnh thực là “waving_hands” (FN kha khá).\n\n        * Support = 110: đủ dùng để đánh giá, không quá ít.\n\n    Nguyên nhân chính:\n\n        * “vẫy tay” và “vỗ tay” (applauding) rất dễ nhầm (như nhìn qua confusion matrix, 16 lần waving_hands → applauding, 25 lần applauding → waving_hands).\n\n        * Các dáng tay, góc chụp, background đa dạng (có thể ngoài trời, trong nhà, gần mặt, xa mặt) – mô hình khó tìm đặc trưng ổn định.\n\n**texting_message (F1=0.5688)**\n\n        * Precision 49.6 %: tức nửa số ảnh model dự đoán “texting_message” đều không phải (FP).\n        \n        * Recall 66.7 %: model bỏ sót ~33 % ảnh thực (FN).\n        \n        * Support = 93: không quá ít.\n\n    Nguyên nhân:\n\n        * “texting_message” (gõ SMS) và “reading” hay “phoning” hay “using_a_computer” đều có cử chỉ tương tự: người cúi nhìn màn hình nhỏ cầm trên tay.\n        \n        * Thiếu cue rõ (màn hình điện thoại vs. mặt bàn phím) vì ảnh tĩnh thường làm mờ chi tiết.\n        \n        * Bối cảnh: có thể ngoài đường, trong nhà, rất khó phân biệt.\n\n**phoning (F1=0.5772)**\n\n    * Precision 61.9 %, Recall 54.1 %.\n    \n    * Rất nhiều nhầm lẫn với “smoking” (24 lần) và “taking_photos” (10 lần).\n    \n    * Khi người cầm điện thoại che gần miệng, giống như cầm điếu thuốc hoặc cầm máy ảnh.\n\n**pouring_liquid (F1=0.6502), smoking (F1=0.6549), taking_photos (F1=0.6605)**\n\nTất cả đều có gestures “tay đưa lên/vào”.\n\n* “pouring_liquid” (rót nước) và “washing_dishes” (rửa bát) đôi khi nhầm lẫn (đã chart confusion 8 lần).\n\n* “smoking” và “phoning” liên tiếp nhầm cho nhau: mỗi bên ~11 lần.\n\n* “taking_photos” (chụp ảnh) vs “phoning” vs “looking_through_telescope/ -microscope”: cả ba đều đưa tay cầm thiết bị sát mặt, khó phân biệt.\n\n**applauding (F1=0.6627)**\n\n    * Precision 72.7 % nhưng Recall chỉ 60.9 %.\n    \n    * Nhầm với “waving_hands” (25 lần) và ngược lại 16 lần.\n\n**washing_dishes (F1=0.6769)**\n\n    * Precision 58.4 % (FP lớn), Recall 80.5 %.\n    \n    * Nhầm với “pouring_liquid” (8 lần) hoặc “cooking” (19 lần).\n    \n    * Background bếp nhà, bàn bồn rửa, dao thớt, nồi chảo dễ trùng với cảnh “cooking”.\n\n**reading (F1=0.7033), drinking (F1=0.7055)**\n\n    * Precision ~75 %, Recall ~66 %.\n\n    * “reading” nhầm với “writing_on_a_book” (23 lần) hoặc “texting_message” (11 lần).\n    \n    * “drinking” nhầm với “pouring_liquid” (11 lần), “phoning” (9 lần).\n    \n    * Hành động cúi sát xuống tô chén hoặc cầm ly lên gần miệng rất dễ nhầm.","metadata":{}},{"cell_type":"markdown","source":"**Kết luận:**\n\n**1. Những hành động “tay đưa lên/vào” dễ nhầm lẫn lẫn nhau**\n\nCác lớp như waving_hands, applauding, phoning, smoking, taking_photos, pouring_liquid đều chia sẻ tư thế tương tự—tay giơ lên gần mặt hoặc sang ngang.\n\nKết quả:\n\n* waving_hands–applauding: nhầm nhau tới 25 và 16 lần.\n\n* phoning–smoking: nhầm nhau 24 lần.\n\n* phoning–taking_photos: nhầm 10 lần.\n\n* pouring_liquid–washing_dishes: nhầm 8 lần.\n\nHậu quả: Precision và Recall của từng lớp đều giảm mạnh (F1 khoảng 0.56–0.66).\n\n**2. Thiếu “cue” rõ rệt để phân biệt khi chỉ có ảnh tĩnh**\n\n* Ví dụ, để phân biệt “texting_message” và “reading” hay “phoning”, model phải nhìn thấy rõ màn hình điện thoại hoặc bàn phím. Ảnh tĩnh thường không đủ chi tiết đó.\n\n* Khi “drinking” và “pouring_liquid” cả hai có cử chỉ đưa ly lên, model dễ bỏ sót (Recall ~66 %) hoặc gán nhầm (Precision ~64 %).\n\n**3. Background (“bối cảnh chung”) góp phần gây nhiễu**\n\n* “washing_dishes” và “cooking” đều diễn ra trong bếp, với bồn rửa, dao thớt, nồi chảo. Model thường chỉ dùng bối cảnh (bếp) để phán đoán, dẫn đến nhầm lẫn 19 lần “cooking → washing_dishes” và 8 lần “washing_dishes → pouring_liquid”.\n\n* “reading”–“writing_on_a_book”: cả hai đều xuất hiện trong khung sách, nên nhầm lẫn 23 lần.\n\n**4. Một số lớp thực hiện động tác đặc trưng rõ ràng, do đó model đã học rất tốt**\n\n* Các hoạt động có tư thế rất đặc trưng như “playing_violin”, “playing_guitar”, “shooting_an_arrow”, “riding_a_bike”, “riding_a_horse” (F1 ≥ 0.95) hầu như không bị nhầm với lớp khác.","metadata":{}},{"cell_type":"markdown","source":"**Hướng cải tiến:**\n\n**1. Tăng cường thông tin pose/hand-object**\n\n* Thêm nhánh pose (keypoint) hoặc heatmap tay để phân biệt cầm khẩu súng, cầm đàn hay cầm ly, cầm điện thoại.\n\n* Dùng một model nhỏ phát hiện “board” (trong reading vs. writing), “phone” vs. “cigarette” vs. “camera” để làm feature phụ.\n\n**2. Định vị vùng quan trọng (ROI)**\n\n* Crop khu vực tay-vật (dựa trên bounding-box phát hiện object) để model chú trọng các chi tiết nhỏ.\n\n* Ví dụ với “phoning”, bắt vùng tay–mặt; với “pouring_liquid”–“washing_dishes”, bắt vùng bồn và cốc chén.\n\n**3. Data augmentation mạnh với các tình huống dễ nhầm**\n\n* Tạo thêm ảnh “vỗ tay”/“vẫy tay” với nhiều background khác nhau, góc chụp nhiều hướng.\n\n* Sinh thêm ảnh “cầm điện thoại” không phải phoning (ví dụ selfie) để giảm FP.\n\n* Thêm ảnh “cầm điếu thuốc” đa dạng, “cầm máy ảnh” ở nhiều góc.\n\n**4. Weighted loss hoặc oversampling**\n\n* Tăng trọng số (loss weight) cho các lớp F1 thấp (ví dụ waving_hands, texting_message, phoning) để model giảm FP/FN.\n\n* Hoặc oversample bộ ảnh của những lớp này, giúp model có nhiều ví dụ hơn để phân biệt.\n\n**5. Fine-tune thêm:**\n\n* Train head + vài block cuối, để thu hẹp khoảng cách domain từ ImageNet.\n\n* Unfreeze toàn bộ backbone, train thêm với learning rate cực thấp, tránh overfit.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score\n\nmodel.eval()\nall_probs = []\nall_labels = []\n\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        imgs = imgs.to(DEVICE)\n        outputs = model(imgs)                 # logits shape (B,40)\n        probs = torch.softmax(outputs, 1).cpu().numpy()  # (B,40)\n        all_probs.append(probs)\n        all_labels.append(labels.numpy())\n\n# Ghép lại thành (N,40) và (N,)\nall_probs = np.concatenate(all_probs, axis=0)   # (N,40)\nall_labels = np.concatenate(all_labels, axis=0) # (N,)\n\n# Tạo mat nhãn one-hot (N,40)\nN, C = all_probs.shape\ny_true_mat = np.zeros((N, C), dtype=np.int32)\ny_true_mat[np.arange(N), all_labels] = 1\n\n# Tính AP cho từng lớp\nAPs = np.zeros(C, dtype=np.float32)\nfor c in range(C):\n    APs[c] = average_precision_score(y_true_mat[:, c], all_probs[:, c])\n\nmAP = APs.mean()\nprint(\"=> mAP = {:.2f}%\".format(mAP * 100.0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Unfreeze more block on backbone  ","metadata":{}},{"cell_type":"code","source":"# ======================= 1. Load model + head-only checkpoint (stage 1) ========================\nmodel = timm.create_model(\"convit_base\", pretrained=False)\nin_features = model.head.in_features\nmodel.head = nn.Linear(in_features, NUM_CLASSES)\nmodel = model.to(DEVICE)\n\nCHECKPOINT_PATH = \"/kaggle/working/best_convit_stanford40.pth\"\ncheckpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\nmodel.load_state_dict(checkpoint)\nprint(\"Loaded checkpoint.\")\n\n\n# ======================= 2. Unfreeze backbone (stage 2) ========================\nfor param in model.parameters():\n    param.requires_grad = False\n\nfor idx in [8, 11]:\n    for param in model.blocks[idx].parameters():\n        param.requires_grad = True\n\nfor param in model.head.parameters():\n    param.requires_grad = True\n\n# ======================= 3. Optimizer, Scheduler, Criterion ========================\nLR = 2e-4\nWEIGHT_DECAY = 3e-2\nTOTAL_EPOCHS = 50\nSTAGE1_EPOCHS = NUM_EPOCHS\nSTAGE2_EPOCHS = TOTAL_EPOCHS - STAGE1_EPOCHS  # = 20\n\noptimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum = 0.9, weight_decay=WEIGHT_DECAY)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=STAGE2_EPOCHS, eta_min=1e-6)\ncriterion = nn.CrossEntropyLoss()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================= 4. Stage 2 Training Loop + Save metrics ========================\ntrain_losses = []\nval_losses   = []\nval_accs     = []\n\nbest_acc = 0.0\npatience = 10\nepochs_no_improve = 0\n\nfor epoch2 in range(1, STAGE2_EPOCHS + 1):\n    epoch = STAGE1_EPOCHS + epoch2\n    model.train()\n    running_train_loss = 0.0\n    total_train = 0\n\n    for imgs, labels in tqdm(train_loader, desc=f\"[Stage2] Epoch {epoch}/{TOTAL_EPOCHS} [Train]\"):\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        bs = imgs.size(0)\n        running_train_loss += loss.item() * bs\n        total_train += bs\n\n    avg_train_loss = running_train_loss / total_train\n    train_losses.append(avg_train_loss)\n    scheduler.step()\n\n    # Validation\n    model.eval()\n    running_val_loss = 0.0\n    total_val = 0\n    correct = 0\n\n    with torch.no_grad():\n        for imgs, labels in tqdm(val_loader, desc=f\"[Stage2] Epoch {epoch}/{TOTAL_EPOCHS} [ Val ]\"):\n            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n\n            bs = imgs.size(0)\n            running_val_loss += loss.item() * bs\n            total_val += bs\n\n            preds = outputs.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n\n    avg_val_loss = running_val_loss / total_val\n    val_losses.append(avg_val_loss)\n\n    val_acc = correct / total_val\n    val_accs.append(val_acc)\n\n    print(\n        f\"Epoch {epoch}/{TOTAL_EPOCHS} | \"\n        f\"Train Loss: {avg_train_loss:.4f} | \"\n        f\"Val Loss: {avg_val_loss:.4f} | \"\n        f\"Val Acc: {val_acc*100:.2f}% | \"\n        f\"LR: {scheduler.get_last_lr()[0]:.6f}\"\n    )\n\n    # Early stopping & lưu model tốt nhất\n    if val_acc > best_acc:\n        best_acc = val_acc\n        epochs_no_improve = 0\n        torch.save(model.state_dict(), \"best_convit_stage2.pth\")\n        print(f\"  → Saved new best model at epoch {epoch} (Val Acc={val_acc*100:.2f}%)\")\n    else:\n        epochs_no_improve += 1\n        if epochs_no_improve >= patience:\n            print(f\"  → No improvement for {patience} epochs → Early stopping at epoch {epoch}\")\n            break\n\nprint(f\"Finished Stage 2. Best Val Acc: {best_acc*100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ======================= 5. Loss & Accuracy ========================\nepochs = np.arange(STAGE1_EPOCHS+1, STAGE1_EPOCHS + len(train_losses) + 1)\n\nplt.figure(figsize=(12, 5))\n\n# Train & Val Loss\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_losses, label=\"Train Loss\")\nplt.plot(epochs, val_losses,   label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Stage 2: Loss over Epochs\")\nplt.legend()\n\n# Val Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(epochs, np.array(val_accs) * 100, label=\"Val Acc (%)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Stage 2: Val Accuracy over Epochs\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# ======================= 6. Confusion Matrix and heatmap ========================\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for imgs, labels in tqdm(val_loader, desc=\"Final Eval [Val]\"):\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        outputs = model(imgs)\n        preds = outputs.argmax(dim=1).cpu().numpy()\n        all_preds.append(preds)\n        all_labels.append(labels.cpu().numpy())\n\nall_preds = np.concatenate(all_preds)\nall_labels = np.concatenate(all_labels)\n\ncm = confusion_matrix(all_labels, all_preds)\n\n# Draw heatmap confusion matrix with matplotlib\nplt.figure(figsize=(10, 8))\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix (Stage 2)\")\nplt.colorbar()\ntick_marks = np.arange(NUM_CLASSES)\n\nplt.xticks(tick_marks, classes, rotation=90, fontsize=7)\nplt.yticks(tick_marks, classes, fontsize=7)\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Khi unfreeze toàn bộ mô hình, hiện tượng overfitting xảy ra trên tập huấn luyện. Do đó, tôi sẽ thử chỉ unfreeze một vài lớp cuối của khối Vision Transformer, như đề xuất trong bài báo, nhằm giúp mô hình học được đặc trưng phân bố hình ảnh liên quan đến hành động.","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nall_probs = []\nall_labels = []\n\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        imgs = imgs.to(DEVICE)\n        outputs = model(imgs)                 # logits shape (B,40)\n        probs = torch.softmax(outputs, 1).cpu().numpy()  # (B,40)\n        all_probs.append(probs)\n        all_labels.append(labels.numpy())\n\n# Ghép lại thành (N,40) và (N,)\nall_probs = np.concatenate(all_probs, axis=0)   # (N,40)\nall_labels = np.concatenate(all_labels, axis=0) # (N,)\n\n# Tạo mat nhãn one-hot (N,40)\nN, C = all_probs.shape\ny_true_mat = np.zeros((N, C), dtype=np.int32)\ny_true_mat[np.arange(N), all_labels] = 1\n\n# Tính AP cho từng lớp\nAPs = np.zeros(C, dtype=np.float32)\nfor c in range(C):\n    APs[c] = average_precision_score(y_true_mat[:, c], all_probs[:, c])\n\nmAP = APs.mean()\nprint(\"=> mAP = {:.2f}%\".format(mAP * 100.0))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}